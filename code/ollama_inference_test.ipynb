{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"VLuAZ3VqN1RI"},"outputs":[{"name":"stdout","output_type":"stream","text":["Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.\n","Your new public key is: \n","\n","ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIMfaHoPJ8FZrSiTARimYOBNameVHAwJrEL0hBJGZY9eC\n","\n","time=2025-07-30T16:45:14.647Z level=INFO source=routes.go:1235 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n","time=2025-07-30T16:45:14.648Z level=INFO source=images.go:476 msg=\"total blobs: 0\"\n","time=2025-07-30T16:45:14.648Z level=INFO source=images.go:483 msg=\"total unused blobs removed: 0\"\n","time=2025-07-30T16:45:14.649Z level=INFO source=routes.go:1288 msg=\"Listening on 127.0.0.1:11434 (version 0.9.6)\"\n","time=2025-07-30T16:45:14.649Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\n","time=2025-07-30T16:45:14.700Z level=INFO source=gpu.go:377 msg=\"no compatible GPUs were discovered\"\n","time=2025-07-30T16:45:14.700Z level=INFO source=types.go:130 msg=\"inference compute\" id=0 library=cpu variant=\"\" compute=\"\" driver=0.0 name=\"\" total=\"12.7 GiB\" available=\"11.7 GiB\"\n"]}],"source":["# 1. Install Ollama CLI\n","# !curl -fsSL https://ollama.com/install.sh | sh\n","\n","# 2. Verify installation\n","!ollama serve\n","!ollama --version\n","\n","# 3. Start the Ollama daemon in the background\n","!nohup ollama daemon \u0026\u003e /dev/null \u0026\n","\n","# 4. Pull GemmaÂ 3 4B\n","!ollama pull gemma3:4b\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMESb0ytfKPtJerv0PzMHDG","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
