{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed6d5f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download NLTK 'punkt' tokenizer...\n",
      "'punkt' tokenizer not found, downloading now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ardjano/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/ardjano/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'punkt' tokenizer download complete.\n",
      "\n",
      "Attempting to download NLTK 'wordnet' corpus...\n",
      "'wordnet' corpus not found, downloading now...\n",
      "'wordnet' corpus download complete.\n",
      "\n",
      "Attempting to download NLTK 'omw-1.4' corpus...\n",
      "'omw-1.4' corpus not found, downloading now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/ardjano/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'omw-1.4' corpus download complete.\n",
      "\n",
      "NLTK data checks complete.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "# This block is often needed for NLTK downloads on some systems\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "print(\"Attempting to download NLTK 'punkt' tokenizer...\")\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"'punkt' tokenizer already downloaded.\")\n",
    "except LookupError: # Corrected from nltk.downloader.LookupError\n",
    "    print(\"'punkt' tokenizer not found, downloading now...\")\n",
    "    nltk.download('punkt')\n",
    "    print(\"'punkt' tokenizer download complete.\")\n",
    "\n",
    "print(\"\\nAttempting to download NLTK 'wordnet' corpus...\")\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "    print(\"'wordnet' corpus already downloaded.\")\n",
    "except LookupError: # Corrected from nltk.downloader.LookupError\n",
    "    print(\"'wordnet' corpus not found, downloading now...\")\n",
    "    nltk.download('wordnet')\n",
    "    print(\"'wordnet' corpus download complete.\")\n",
    "\n",
    "print(\"\\nAttempting to download NLTK 'omw-1.4' corpus...\")\n",
    "try:\n",
    "    nltk.data.find('corpora/omw-1.4')\n",
    "    print(\"'omw-1.4' corpus already downloaded.\")\n",
    "except LookupError: # Corrected from nltk.downloader.LookupError\n",
    "    print(\"'omw-1.4' corpus not found, downloading now...\")\n",
    "    nltk.download('omw-1.4')\n",
    "    print(\"'omw-1.4' corpus download complete.\")\n",
    "\n",
    "print(\"\\nNLTK data checks complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af32d8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to find/download 'punkt'...\n",
      "'punkt' already downloaded.\n",
      "Attempting to find/download 'wordnet'...\n",
      "'wordnet' not found, downloading now...\n",
      "'wordnet' download complete.\n",
      "Attempting to find/download 'omw-1.4'...\n",
      "'omw-1.4' not found, downloading now...\n",
      "'omw-1.4' download complete.\n",
      "Attempting to find/download 'punkt_tab'...\n",
      "'punkt_tab' already downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ardjano/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ardjano/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk_data_items = ['punkt', 'wordnet', 'omw-1.4', 'punkt_tab'] # <--- ADDED: 'punkt_tab'\n",
    "\n",
    "for item in nltk_data_items:\n",
    "    print(f\"Attempting to find/download '{item}'...\")\n",
    "    try:\n",
    "        # For 'punkt_tab', NLTK's find path is slightly different, but the download command is usually `nltk.download('punkt_tab')`\n",
    "        if item == 'punkt_tab':\n",
    "            # Punkt_tab is a specific sub-package often found under tokenizers/punkt/\n",
    "            # Direct find might look for 'tokenizers/punkt_tab/english' as per your error\n",
    "            # We'll just try to download it directly if it fails.\n",
    "            pass # We'll rely on the download catching it\n",
    "        else:\n",
    "            nltk.data.find(f'tokenizers/{item}') if 'punkt' in item else (nltk.data.find(f'corpora/{item}') if 'wordnet' in item or 'omw' in item else nltk.data.find(item))\n",
    "        print(f\"'{item}' already downloaded.\")\n",
    "    except LookupError: # Catches if resource is not found\n",
    "        print(f\"'{item}' not found, downloading now...\")\n",
    "        try:\n",
    "            nltk.download(item) # This should handle 'punkt_tab' as well\n",
    "            print(f\"'{item}' download complete.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading '{item}': {e}\")\n",
    "            print(f\"Please try running 'nltk.download('{item}')' manually in your Python environment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd0b85ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.13 environment at: /home/ardjano/.pyenv/versions/3.11.13/envs/vllm-env\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m58 packages\u001b[0m \u001b[2min 610ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rouge-score\u001b[2m==0.1.2\u001b[0m                                         \n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rouge-score\u001b[2m==0.1.2\u001b[0m                                 \u001b[1A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rouge-score\u001b[2m==0.1.2\u001b[0m----\u001b[0m\u001b[0m     0 B/1.44 MiB            \u001b[2A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rouge-score\u001b[2m==0.1.2\u001b[0m----\u001b[0m\u001b[0m     0 B/1.44 MiB            \u001b[2A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2mbert-score          \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/59.70 KiB\n",
      "\u001b[2K\u001b[3A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rouge-score\u001b[2m==0.1.2\u001b[0m----\u001b[0m\u001b[0m     0 B/1.44 MiB            \u001b[3A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2mbert-score          \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/59.70 KiB\n",
      "\u001b[2K\u001b[3A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rouge-score\u001b[2m==0.1.2\u001b[0m----\u001b[0m\u001b[0m 16.00 KiB/1.44 MiB          \u001b[3A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2mbert-score          \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/59.70 KiB\n",
      "\u001b[2K\u001b[3A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rouge-score\u001b[2m==0.1.2\u001b[0m----\u001b[0m\u001b[0m 32.00 KiB/1.44 MiB          \u001b[3A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2mbert-score          \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/59.70 KiB\n",
      "\u001b[2K\u001b[3A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rouge-score\u001b[2m==0.1.2\u001b[0m----\u001b[0m\u001b[0m 48.00 KiB/1.44 MiB          \u001b[3A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2mbert-score          \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/59.70 KiB\n",
      "\u001b[2K\u001b[3A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rouge-score\u001b[2m==0.1.2\u001b[0m----\u001b[0m\u001b[0m 64.00 KiB/1.44 MiB          \u001b[3A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2mbert-score          \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/59.70 KiB\n",
      "\u001b[2K\u001b[3A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rouge-score\u001b[2m==0.1.2\u001b[0m----\u001b[0m\u001b[0m 76.57 KiB/1.44 MiB          \u001b[3A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2mbert-score          \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/59.70 KiB\n",
      "\u001b[2K\u001b[3A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rouge-score\u001b[2m==0.1.2\u001b[0m----\u001b[0m\u001b[0m 92.57 KiB/1.44 MiB          \u001b[3A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2mbert-score          \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/59.70 KiB\n",
      "\u001b[2K\u001b[3A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rouge-score\u001b[2m==0.1.2\u001b[0m----\u001b[0m\u001b[0m 108.57 KiB/1.44 MiB         \u001b[3A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2mbert-score          \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 16.00 KiB/59.70 KiB\n",
      "\u001b[2K\u001b[3A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rouge-score\u001b[2m==0.1.2\u001b[0m----\u001b[0m\u001b[0m 108.57 KiB/1.44 MiB         \u001b[3A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2mbert-score          \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 16.00 KiB/59.70 KiB\n",
      "\u001b[2K\u001b[3A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rouge-score\u001b[2m==0.1.2\u001b[0m----\u001b[0m\u001b[0m 124.57 KiB/1.44 MiB         \u001b[3A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2mbert-score          \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 32.00 KiB/59.70 KiB\n",
      "\u001b[2K\u001b[3A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rouge-score\u001b[2m==0.1.2\u001b[0m----\u001b[0m\u001b[0m 124.57 KiB/1.44 MiB         \u001b[3A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2mbert-score          \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 32.00 KiB/59.70 KiB\n",
      "\u001b[2K\u001b[3A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rouge-score\u001b[2m==0.1.2\u001b[0m----\u001b[0m\u001b[0m 140.57 KiB/1.44 MiB         \u001b[3A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2mbert-score          \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 48.00 KiB/59.70 KiB\n",
      "\u001b[2K\u001b[3A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rouge-score\u001b[2m==0.1.2\u001b[0m----\u001b[0m\u001b[0m 140.57 KiB/1.44 MiB         \u001b[3A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2mbert-score          \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 48.00 KiB/59.70 KiB\n",
      "\u001b[2K\u001b[3A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rouge-score\u001b[2m==0.1.2\u001b[0m----\u001b[0m\u001b[0m 156.57 KiB/1.44 MiB         \u001b[3A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rouge-score\u001b[2m==0.1.2\u001b[0m----\u001b[0m\u001b[0m 156.57 KiB/1.44 MiB         \u001b[2A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rouge-score\u001b[2m==0.1.2\u001b[0m----\u001b[0m\u001b[0m 460.57 KiB/1.44 MiB         \u001b[2A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rouge-score\u001b[2m==0.1.2\u001b[0m----\u001b[0m\u001b[0m 1.01 MiB/1.44 MiB           \u001b[2A\n",
      "\u001b[2K\u001b[1A      \u001b[32m\u001b[1mBuilt\u001b[0m\u001b[39m rouge-score\u001b[2m==0.1.2\u001b[0m                                 \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 195ms\u001b[0m\u001b[0m                                             \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m8 packages\u001b[0m \u001b[2min 7ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbert-score\u001b[0m\u001b[2m==0.3.13\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcontourpy\u001b[0m\u001b[2m==1.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcycler\u001b[0m\u001b[2m==0.12.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfonttools\u001b[0m\u001b[2m==4.59.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mkiwisolver\u001b[0m\u001b[2m==1.4.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.10.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnltk\u001b[0m\u001b[2m==3.9.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrouge-score\u001b[0m\u001b[2m==0.1.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install bert_score nltk rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57db922e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/ardjano/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b431991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Calculating metrics for vLLM_Generated ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19830a9e998044728155d087109dce75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing vLLM_Generated rows:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardjano/.pyenv/versions/vllm-env/lib/python3.11/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ardjano/.pyenv/versions/vllm-env/lib/python3.11/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ardjano/.pyenv/versions/vllm-env/lib/python3.11/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics calculated for vLLM_Generated. Saved to vLLM_Generated_scores.csv\n",
      "Average scores for vLLM_Generated:\n",
      "  BERTScore: 0.5297\n",
      "  BLEU: 0.0036\n",
      "  ROUge-L: 0.1491\n"
     ]
    }
   ],
   "source": [
    "from bert_score import BERTScorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "# from nltk.translate.nist_score import corpus_nist # Not used in your provided script\n",
    "# from nltk.translate.meteor_score import meteor_score # Commented out\n",
    "from nltk import word_tokenize\n",
    "from tqdm.auto import tqdm # Import tqdm for progress bars\n",
    "\n",
    "# Ensure NLTK data is downloaded if you haven't already\n",
    "# import nltk\n",
    "# try:\n",
    "#     nltk.data.find('tokenizers/punkt')\n",
    "# except nltk.downloader.DownloadError:\n",
    "#     nltk.download('punkt')\n",
    "# try:\n",
    "#     nltk.data.find('corpora/wordnet')\n",
    "# except nltk.downloader.DownloadError:\n",
    "#     nltk.download('wordnet')\n",
    "# try:\n",
    "#     nltk.data.find('corpora/omw-1.4')\n",
    "# except nltk.downloader.DownloadError:\n",
    "#     nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "pattern = re.compile(r\"[^A-Za-z0-9 '’]\")\n",
    "\n",
    "def calculate_bertscore(candidate, ground_truth):\n",
    "    # BERTScorer can be initialized once outside the loop for efficiency\n",
    "    # but for simplicity in a function, we'll keep it here.\n",
    "    # For better performance on large datasets, initialize it globally or pass it.\n",
    "    scorer = BERTScorer(model_type='bert-base-uncased', device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    P, R, F1 = scorer.score([candidate], [ground_truth])\n",
    "    return F1.mean().item() # .item() to get scalar from tensor\n",
    "\n",
    "def calculate_bleu(candidate, standard1, standard2):\n",
    "  # Standardize and tokenize for BLEU\n",
    "  candidate_tokens = word_tokenize(re.sub(pattern, \"\", str(candidate)))\n",
    "  reference1_tokens = word_tokenize(re.sub(pattern, \"\", str(standard1)))\n",
    "  reference2_tokens = word_tokenize(re.sub(pattern, \"\", str(standard2)))\n",
    "\n",
    "  # BLEU score requires list of references.\n",
    "  # If you only have one true response, you can duplicate it for multiple references.\n",
    "  # Your current setup passes ground_truth twice for standard1 and standard2.\n",
    "  bleu_score = sentence_bleu([reference1_tokens, reference2_tokens], candidate_tokens)\n",
    "  return bleu_score\n",
    "\n",
    "def calculate_rouge_L(candidate, ground_truth):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    # Ensure inputs are strings\n",
    "    rouge_L_score = scorer.score(str(candidate), str(ground_truth))\n",
    "    return rouge_L_score['rougeL'].fmeasure\n",
    "\n",
    "# def calculate_meteor(candidate, ground_truth):\n",
    "#     # Ensure inputs are strings and tokenize\n",
    "#     return meteor_score([word_tokenize(str(ground_truth))], word_tokenize(str(candidate)))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define your input CSV file paths\n",
    "    # Assuming these are the CSVs generated by your vLLM script (e.g., 'vllm_model_test_responses_full_testset.csv')\n",
    "    # and other baseline models if you have them.\n",
    "    # Adjust these paths as per your file structure.\n",
    "    # For this example, let's assume we are evaluating the single vLLM output:\n",
    "    vllm_output_csv = \"vllm_model_test_responses_finetuned_gemma3-4b.csv\" # Update to your actual vLLM output file\n",
    "    # deepseek_df_path = \"xuelong/deepseek_baseline1.csv\" # Example from your original script\n",
    "    # gemma_df_path = \"xuelong/gemma_baseline1.csv\"       # Example from your original script\n",
    "    # qwen_df_path = \"xuelong/qwen_baseline1.csv\"         # Example from your original script\n",
    "\n",
    "    # List of dataframes to process\n",
    "    dataframes_to_process = {\n",
    "        \"vLLM_Generated\": pd.read_csv(vllm_output_csv),\n",
    "        # Add other models if you want to evaluate them too:\n",
    "        # \"DeepSeek_Baseline\": pd.read_csv(deepseek_df_path),\n",
    "        # \"Gemma_Baseline\": pd.read_csv(gemma_df_path),\n",
    "        # \"Qwen_Baseline\": pd.read_csv(qwen_df_path),\n",
    "    }\n",
    "\n",
    "    for df_name, df_data in dataframes_to_process.items():\n",
    "        print(f\"\\n--- Calculating metrics for {df_name} ---\")\n",
    "\n",
    "        # Ensure 'BERTScore' column exists and initialize it to 0.0\n",
    "        # This prevents KeyError if the column doesn't exist.\n",
    "        df_data['BERTScore'] = 0.0\n",
    "        df_data['BLEU'] = 0.0\n",
    "        df_data['ROUGE-L'] = 0.0\n",
    "        # df_data['METEOR'] = 0.0 # Commented out\n",
    "\n",
    "        # Use tqdm for a progress bar during iteration\n",
    "        for i, row in tqdm(df_data.iterrows(), total=len(df_data), desc=f\"Processing {df_name} rows\"):\n",
    "            # !!! IMPORTANT: Adjust column names here to match your CSV structure !!!\n",
    "            # Based on previous discussions, 'Generated_Response' is the candidate\n",
    "            # and 'True_Response' is the ground truth.\n",
    "            candidate = row['Generated_Response']\n",
    "            ground_truth = row['True_Response']\n",
    "\n",
    "            # Handle potential non-string values gracefully\n",
    "            candidate = str(candidate) if pd.isna(candidate) else candidate\n",
    "            ground_truth = str(ground_truth) if pd.isna(ground_truth) else ground_truth\n",
    "\n",
    "            try:\n",
    "                bertscore = calculate_bertscore(candidate, ground_truth)\n",
    "            except Exception as e:\n",
    "                bertscore = 0.0\n",
    "                print(f\"BERTScore error at index {i} for {df_name}: {e}\") # Debugging\n",
    "\n",
    "            try:\n",
    "                # For BLEU, if you only have one true response, you can still pass it twice\n",
    "                # as the two \"standards\" if your function signature expects two.\n",
    "                # calculate_bleu(candidate, ground_truth, ground_truth) is valid.\n",
    "                bleu = calculate_bleu(candidate, ground_truth, ground_truth)\n",
    "            except Exception as e:\n",
    "                bleu = 0.0\n",
    "                print(f\"BLEU error at index {i} for {df_name}: {e}\") # Debugging\n",
    "\n",
    "            try:\n",
    "                rouge_L = calculate_rouge_L(candidate, ground_truth)\n",
    "            except Exception as e:\n",
    "                rouge_L = 0.0\n",
    "                print(f\"ROUGE-L error at index {i} for {df_name}: {e}\") # Debugging\n",
    "\n",
    "            # try: # Commented out\n",
    "            #     meteor = calculate_meteor(candidate, ground_truth)\n",
    "            # except Exception as e:\n",
    "            #     meteor = 0.0\n",
    "            #     print(f\"METEOR error at index {i} for {df_name}: {e}\") # Debugging\n",
    "\n",
    "            df_data.at[i, 'BERTScore'] = bertscore\n",
    "            df_data.at[i, 'BLEU'] = bleu\n",
    "            df_data.at[i, 'ROUGE-L'] = rouge_L\n",
    "            # df_data.at[i, 'METEOR'] = meteor # Commented out\n",
    "\n",
    "        # Define output file path\n",
    "        output_csv_path = f\"{df_name}_scores.csv\" # Adjust output path as needed, e.g., \"results/{df_name}_scores.csv\"\n",
    "        df_data.to_csv(output_csv_path, index=False)\n",
    "\n",
    "        print(f\"Metrics calculated for {df_name}. Saved to {output_csv_path}\")\n",
    "        print(f\"Average scores for {df_name}:\")\n",
    "        print(f\"  BERTScore: {df_data['BERTScore'].mean():.4f}\")\n",
    "        print(f\"  BLEU: {df_data['BLEU'].mean():.4f}\")\n",
    "        print(f\"  ROUge-L: {df_data['ROUGE-L'].mean():.4f}\")\n",
    "        # print(f\"  METEOR: {df_data['METEOR'].mean():.4f}\") # Commented out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
