{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcf886b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install --upgrade pip -q\n",
    "# !pip install uv -qU\n",
    "# !uv pip install \"numpy<2.3\" \"transformers<=4.53.2\" datasets tensorboard openai hf_transfer accelerate pillow scikit-learn pymupdf google.generativeai flashinfer-python huggingface_hub vllm ipywidgets -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bf92976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sw/arch/RHEL9/EB_production/2024/software/CUDA/12.6.0/bin/nvcc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# The path where 'which nvcc' found the compiler in your terminal\n",
    "cuda_path = '/sw/arch/RHEL9/EB_production/2024/software/CUDA/12.6.0/bin'\n",
    "\n",
    "# Prepend the CUDA path to the environment variable\n",
    "os.environ['PATH'] = f\"{cuda_path}:{os.environ['PATH']}\"\n",
    "\n",
    "# Verify the change\n",
    "!which nvcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "692e4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !uv pip install datasets tensorboard openai hf_transfer accelerate pillow -qU\n",
    "# !uv pip install scikit-learn pymupdf -qU\n",
    "# !uv pip install google.generativeai  -qU # for gemini\n",
    "# !uv pip install flashinfer-python  -qU # helpful for speeding up vllm\n",
    "# !uv pip install huggingface_hub -qU\n",
    "# !uv pip install vllm -qU\n",
    "# !uv pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30ab1be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HF_HOME'] = '/scratch-shared/amark/huggingface_cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e47826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CPU cores available: 64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Get the total number of CPU cores available in your Colab instance\n",
    "total_cores = os.cpu_count()\n",
    "print(f\"Total CPU cores available: {total_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99e50c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder, login\n",
    "\n",
    "# Check if a token is already saved\n",
    "if HfFolder.get_token() is None:\n",
    "    login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e06f6abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_slug = \"google/gemma-3-4b-it\"\n",
    "model_slug = \"Ardjano/gemma3-27b-finetune1-merged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "861dfe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"../data/humanandllm.csv\")\n",
    "df = df[['Context', 'Response']]\n",
    "\n",
    "trainset, testset = train_test_split(df, test_size=0.057, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60c9c1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have no real friends. I have a girlfriend who irritates me but loves me to death. I push her away and pushes me away. Weâ€™re going through a breakup, and I have nobody.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(testset.iloc[1].Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d93d25b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m dtype = \u001b[33m\"\u001b[39m\u001b[33mbfloat16\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m      6\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mT4\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m torch.cuda.get_device_name(\u001b[32m0\u001b[39m):\n\u001b[32m      7\u001b[39m     dtype = \u001b[33m\"\u001b[39m\u001b[33mfloat16\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vllm_env/lib/python3.11/site-packages/torch/cuda/__init__.py:174\u001b[39m, in \u001b[36mis_available\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m device_count() > \u001b[32m0\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# The default availability inspection never throws and returns 0 if the driver is missing or can't\u001b[39;00m\n\u001b[32m    172\u001b[39m     \u001b[38;5;66;03m# be initialized. This uses the CUDA Runtime API `cudaGetDeviceCount` which in turn initializes the CUDA Driver\u001b[39;00m\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# API via `cuInit`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_getDeviceCount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m > \u001b[32m0\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = \"bfloat16\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  if \"T4\" in torch.cuda.get_device_name(0):\n",
    "    dtype = \"float16\"\n",
    "else:\n",
    "  dtype = \"float32\"\n",
    "\n",
    "print(f\"dtype: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99822fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_slug)\n",
    "# print(f\"Tokenizer type: {type(tokenizer)}\")\n",
    "# print(f\"Tokenizer model name: {tokenizer.name_or_path}\")\n",
    "\n",
    "# The original model with the correct tokenizer and chat template\n",
    "tokenizer_source = \"google/gemma-3-27b-it\"\n",
    "\n",
    "# Load the tokenizer from the original source\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a1a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_generation_prompt(context_text, tokenizer_obj):\n",
    "    \"\"\"\n",
    "    Formats a single context (user's message) into the chat template for model generation.\n",
    "    \"\"\"\n",
    "    # Create the conversation structure directly with the provided context_text\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": context_text},\n",
    "    ]\n",
    "    # For generation, add_generation_prompt should be True\n",
    "    formatted_prompt = tokenizer_obj.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return formatted_prompt\n",
    "\n",
    "prompts = []\n",
    "original_prompts = []\n",
    "true_responses = []\n",
    "\n",
    "for i, row in testset.iterrows():\n",
    "    context_text = row['Context']\n",
    "    true_response_text = row['Response']\n",
    "\n",
    "    formatted_input_string = format_generation_prompt(context_text, tokenizer)\n",
    "\n",
    "    prompts.append(formatted_input_string)\n",
    "    original_prompts.append(context_text)\n",
    "    true_responses.append(true_response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb202bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_slug,\n",
    "    use_fast=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e6b4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 05:15:36 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 09-26 05:15:49 [config.py:823] This model supports multiple tasks: {'reward', 'score', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 09-26 05:15:49 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=16384.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-26 05:15:51 [utils.py:2597] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "WARNING 09-26 05:15:53 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\n",
      "INFO 09-26 05:15:56 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 09-26 05:15:59 [core.py:455] Waiting for init message from front-end.\n",
      "INFO 09-26 05:15:59 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='Ardjano/gemma3-27b-finetune1-merged', speculative_config=None, tokenizer='Ardjano/gemma3-27b-finetune1-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Ardjano/gemma3-27b-finetune1-merged, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 09-26 05:16:00 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14572f300e90>\n",
      "INFO 09-26 05:16:00 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 05:16:08 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 09-26 05:16:08 [gpu_model_runner.py:1595] Starting to load model Ardjano/gemma3-27b-finetune1-merged...\n",
      "INFO 09-26 05:16:08 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "INFO 09-26 05:16:08 [cuda.py:252] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 05:16:09 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "INFO 09-26 05:16:09 [weight_utils.py:308] Time spent downloading weights for Ardjano/gemma3-27b-finetune1-merged: 0.588399 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   8% Completed | 1/12 [00:04<00:50,  4.60s/it]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 2/12 [00:09<00:46,  4.68s/it]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 3/12 [00:14<00:42,  4.68s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 4/12 [00:18<00:37,  4.68s/it]\n",
      "Loading safetensors checkpoint shards:  42% Completed | 5/12 [00:23<00:32,  4.62s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 6/12 [00:23<00:19,  3.27s/it]\n",
      "Loading safetensors checkpoint shards:  58% Completed | 7/12 [00:28<00:18,  3.66s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 8/12 [00:32<00:15,  3.95s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 9/12 [00:37<00:12,  4.15s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 10/12 [00:42<00:08,  4.29s/it]\n",
      "Loading safetensors checkpoint shards:  92% Completed | 11/12 [00:46<00:04,  4.38s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 12/12 [00:51<00:00,  4.43s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 12/12 [00:51<00:00,  4.27s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 05:17:01 [default_loader.py:272] Loading weights took 51.27 seconds\n",
      "INFO 09-26 05:17:01 [gpu_model_runner.py:1624] Model loading took 51.4479 GiB and 52.501311 seconds\n",
      "INFO 09-26 05:17:01 [gpu_model_runner.py:1978] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 64 image items of the maximum feature size.\n",
      "INFO 09-26 05:17:19 [backends.py:462] Using cache directory: /home/amark/.cache/vllm/torch_compile_cache/8d6547b87e/rank_0_0 for vLLM's torch.compile\n",
      "INFO 09-26 05:17:19 [backends.py:472] Dynamo bytecode transform time: 15.84 s\n",
      "INFO 09-26 05:17:34 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 13.261 s\n",
      "INFO 09-26 05:17:41 [monitor.py:34] torch.compile takes 15.84 s in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 05:18:29 [gpu_worker.py:227] Available KV cache memory: 25.10 GiB\n",
      "WARNING 09-26 05:18:29 [kv_cache_utils.py:830] Add 8 padding layers, may waste at most 15.38% KV cache memory\n",
      "INFO 09-26 05:18:29 [kv_cache_utils.py:870] GPU KV cache size: 46,992 tokens\n",
      "INFO 09-26 05:18:29 [kv_cache_utils.py:874] Maximum concurrency for 8,192 tokens per request: 5.73x\n",
      "INFO 09-26 05:19:02 [gpu_model_runner.py:2048] Graph capturing finished in 33 secs, took 0.81 GiB\n",
      "INFO 09-26 05:19:02 [core.py:171] init engine (profile, create kv cache, warmup model) took 121.32 seconds\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=1024,\n",
    "    top_k=64,\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    "    min_p=0.0\n",
    ")\n",
    "\n",
    "model = LLM(\n",
    "    model=model_slug,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    dtype=dtype,\n",
    "    max_model_len=8192\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9679ceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip freeze --color never > eval_packages.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ea21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6a802cf9744b4aae0cb87b3c2f4769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda6cebb6b864e20878b73619d4d4ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700d8e91dbee49f3a57088712dec349a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing outputs:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 200 generated responses.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "outputs = model.generate(prompts, sampling_params)\n",
    "\n",
    "generated_responses_data = []\n",
    "\n",
    "# Process the outputs\n",
    "for i, output in tqdm(enumerate(outputs), total=len(outputs), desc=\"Processing outputs\"):\n",
    "    generated_text = output.outputs[0].text.strip()\n",
    "\n",
    "    generated_responses_data.append({\n",
    "        \"Context\": original_prompts[i],\n",
    "        \"True_Response\": true_responses[i],\n",
    "        \"Generated_Response\": generated_text\n",
    "    })\n",
    "\n",
    "print(f\"Finished processing {len(generated_responses_data)} generated responses.\")\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "responses_df = pd.DataFrame(generated_responses_data)\n",
    "output_csv_path = \"vllm_model_test_responses_finetuned_gemma3-27b.csv\"\n",
    "responses_df.to_csv(output_csv_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
