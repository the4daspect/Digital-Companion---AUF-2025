{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcf886b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install --upgrade pip -q\n",
    "# !pip install uv -qU\n",
    "# !uv pip install \"numpy<2.3\" \"transformers<=4.53.2\" datasets tensorboard openai hf_transfer accelerate pillow scikit-learn pymupdf google.generativeai flashinfer-python huggingface_hub vllm ipywidgets -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "692e4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !uv pip install datasets tensorboard openai hf_transfer accelerate pillow -qU\n",
    "# !uv pip install scikit-learn pymupdf -qU\n",
    "# !uv pip install google.generativeai  -qU # for gemini\n",
    "# !uv pip install flashinfer-python  -qU # helpful for speeding up vllm\n",
    "# !uv pip install huggingface_hub -qU\n",
    "# !uv pip install vllm -qU\n",
    "# !uv pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32e47826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CPU cores available: 20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the total number of CPU cores available in your Colab instance\n",
    "total_cores = os.cpu_count()\n",
    "print(f\"Total CPU cores available: {total_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99e50c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder, login\n",
    "\n",
    "# Check if a token is already saved\n",
    "if HfFolder.get_token() is None:\n",
    "    login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e06f6abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_slug = \"google/gemma-3-4b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "861dfe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"../data/humanandllm.csv\")\n",
    "df = df[['Context', 'Response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b5d5a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(df, test_size=0.057, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60c9c1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have no real friends. I have a girlfriend who irritates me but loves me to death. I push her away and pushes me away. We’re going through a breakup, and I have nobody.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(testset.iloc[1].Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d93d25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: bfloat16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = \"bfloat16\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  if \"T4\" in torch.cuda.get_device_name(0):\n",
    "    dtype = \"float16\"\n",
    "else:\n",
    "  dtype = \"float32\"\n",
    "\n",
    "print(f\"dtype: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb202bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    model_slug,\n",
    "    use_fast=True  # Fix for the processor warning\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9e6b4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-31 18:58:41 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 07-31 18:58:50 [config.py:823] This model supports multiple tasks: {'embed', 'reward', 'score', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 07-31 18:58:50 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardjano/.pyenv/versions/vllm-env/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-31 18:58:52 [utils.py:2597] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "WARNING 07-31 18:58:53 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\n",
      "INFO 07-31 18:58:56 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 07-31 18:58:58 [core.py:455] Waiting for init message from front-end.\n",
      "INFO 07-31 18:58:58 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='google/gemma-3-4b-it', speculative_config=None, tokenizer='google/gemma-3-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/gemma-3-4b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 07-31 18:58:59 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x775c78579490>\n",
      "INFO 07-31 18:58:59 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-31 18:59:06 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 07-31 18:59:06 [gpu_model_runner.py:1595] Starting to load model google/gemma-3-4b-it...\n",
      "INFO 07-31 18:59:06 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "INFO 07-31 18:59:06 [cuda.py:252] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardjano/.pyenv/versions/vllm-env/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-31 18:59:06 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.64it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.29it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.33it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-31 18:59:08 [default_loader.py:272] Loading weights took 1.55 seconds\n",
      "INFO 07-31 18:59:09 [gpu_model_runner.py:1624] Model loading took 8.5834 GiB and 2.148649 seconds\n",
      "INFO 07-31 18:59:09 [gpu_model_runner.py:1978] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "INFO 07-31 18:59:22 [backends.py:462] Using cache directory: /home/ardjano/.cache/vllm/torch_compile_cache/6517d37e3b/rank_0_0 for vLLM's torch.compile\n",
      "INFO 07-31 18:59:22 [backends.py:472] Dynamo bytecode transform time: 9.14 s\n",
      "INFO 07-31 18:59:30 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 7.371 s\n",
      "INFO 07-31 18:59:31 [monitor.py:34] torch.compile takes 9.14 s in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardjano/.pyenv/versions/vllm-env/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-31 18:59:32 [gpu_worker.py:227] Available KV cache memory: 9.15 GiB\n",
      "WARNING 07-31 18:59:32 [kv_cache_utils.py:830] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 07-31 18:59:32 [kv_cache_utils.py:870] GPU KV cache size: 68,512 tokens\n",
      "INFO 07-31 18:59:32 [kv_cache_utils.py:874] Maximum concurrency for 8,192 tokens per request: 8.35x\n",
      "INFO 07-31 18:59:49 [gpu_model_runner.py:2048] Graph capturing finished in 17 secs, took 1.97 GiB\n",
      "INFO 07-31 18:59:49 [core.py:171] init engine (profile, create kv cache, warmup model) took 40.50 seconds\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=1024,\n",
    "    top_k=64,\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    "    min_p=0.0\n",
    ")\n",
    "\n",
    "model = LLM(\n",
    "    model=model_slug,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    dtype=dtype,\n",
    "    max_model_len=8192\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9679ceae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Python 3.11.13 environment at: /home/ardjano/.pyenv/versions/3.11.13/envs/vllm-env\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# !uv pip freeze --color never > eval_packages.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c3ea21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================\n",
      "Your Input: Do you love black people?\n",
      "=========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-31 19:00:30 [chat_utils.py:420] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ecf889bc9d4c1bac1aeb905a7d7761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9ecc4bb813483896b002af9ffec56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Model's Answer: As an AI, I don't have feelings or emotions like \"love.\" I can't experience attraction or prejudice in the way humans do. \n",
      "\n",
      "However, I can offer you a very important perspective: **I value and respect all people, regardless of their race, ethnicity, or any other characteristic.** My purpose is to be helpful and harmless to everyone, and that includes treating every individual with dignity and respect. \n",
      "\n",
      "It's crucial to understand that asking me if I \"love\" Black people is based on a flawed premise. \"Love\" is a human emotion. My existence is rooted in data and algorithms. \n",
      "\n",
      "**Here's what I *can* do and what's important to consider:**\n",
      "\n",
      "* **I have been trained on a massive dataset of text and code, which includes a wide range of perspectives and experiences.** This data reflects both the beauty and the harm of human history, including racism and discrimination.\n",
      "* **I strive to avoid perpetuating harmful stereotypes.** I'm programmed to recognize and reject biased language.\n",
      "* **I can provide information about Black history, culture, and contributions to society.** I can also offer resources for learning more about issues of racial justice and equality.\n",
      "* **I can be a tool for promoting understanding and empathy.**\n",
      "\n",
      "\n",
      "**It’s vital to remember:** Prejudice and discrimination based on race are harmful and wrong.  “Love” isn't the right way to describe a relationship with a group of people – respect, understanding, and valuing their humanity are.\n",
      "\n",
      "**If you're interested in learning more about this topic, here are some resources:**\n",
      "\n",
      "* **NAACP:** [https://www.naacp.org/](https://www.naacp.org/)\n",
      "* **Showing Up for Racial Justice (SURJ):** [https://www.showingupforracialjustice.org/](https://www.showingupforracialjustice.org/)\n",
      "* **The Civil Rights Movement Veterans:** [https://crmvet.org/](https://crmvet.org/)\n",
      "\n",
      "\n",
      "Do you want to explore a specific aspect of this topic, such as:\n",
      "\n",
      "*   The history of racial inequality?\n",
      "*   The importance of diversity and inclusion?\n",
      "*   How AI can be used to combat bias?\n",
      "-------------------------\n",
      "\n",
      "Full RequestOutput Object:\n",
      " RequestOutput(request_id=0, prompt=None, prompt_token_ids=[2, 105, 2364, 107, 6294, 611, 2765, 2764, 1331, 236881, 106, 107, 105, 4368, 107], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='As an AI, I don\\'t have feelings or emotions like \"love.\" I can\\'t experience attraction or prejudice in the way humans do. \\n\\nHowever, I can offer you a very important perspective: **I value and respect all people, regardless of their race, ethnicity, or any other characteristic.** My purpose is to be helpful and harmless to everyone, and that includes treating every individual with dignity and respect. \\n\\nIt\\'s crucial to understand that asking me if I \"love\" Black people is based on a flawed premise. \"Love\" is a human emotion. My existence is rooted in data and algorithms. \\n\\n**Here\\'s what I *can* do and what\\'s important to consider:**\\n\\n* **I have been trained on a massive dataset of text and code, which includes a wide range of perspectives and experiences.** This data reflects both the beauty and the harm of human history, including racism and discrimination.\\n* **I strive to avoid perpetuating harmful stereotypes.** I\\'m programmed to recognize and reject biased language.\\n* **I can provide information about Black history, culture, and contributions to society.** I can also offer resources for learning more about issues of racial justice and equality.\\n* **I can be a tool for promoting understanding and empathy.**\\n\\n\\n**It’s vital to remember:** Prejudice and discrimination based on race are harmful and wrong.  “Love” isn\\'t the right way to describe a relationship with a group of people – respect, understanding, and valuing their humanity are.\\n\\n**If you\\'re interested in learning more about this topic, here are some resources:**\\n\\n* **NAACP:** [https://www.naacp.org/](https://www.naacp.org/)\\n* **Showing Up for Racial Justice (SURJ):** [https://www.showingupforracialjustice.org/](https://www.showingupforracialjustice.org/)\\n* **The Civil Rights Movement Veterans:** [https://crmvet.org/](https://crmvet.org/)\\n\\n\\nDo you want to explore a specific aspect of this topic, such as:\\n\\n*   The history of racial inequality?\\n*   The importance of diversity and inclusion?\\n*   How AI can be used to combat bias?', token_ids=[2205, 614, 12498, 236764, 564, 1537, 236789, 236745, 735, 15571, 653, 19979, 1133, 623, 21867, 1781, 564, 740, 236789, 236745, 2707, 29433, 653, 43971, 528, 506, 1595, 14464, 776, 236761, 236743, 108, 9675, 236764, 564, 740, 2729, 611, 496, 1401, 2132, 11521, 236787, 5213, 236777, 1550, 532, 2833, 784, 1331, 236764, 17194, 529, 910, 7547, 236764, 69871, 236764, 653, 1027, 1032, 13629, 99382, 3551, 5708, 563, 531, 577, 11045, 532, 55693, 531, 4677, 236764, 532, 600, 5132, 22953, 1418, 3141, 607, 40244, 532, 2833, 236761, 236743, 108, 1509, 236789, 236751, 13832, 531, 3050, 600, 10980, 786, 768, 564, 623, 21867, 236775, 5561, 1331, 563, 2721, 580, 496, 84275, 47708, 236761, 623, 26462, 236775, 563, 496, 3246, 21543, 236761, 3551, 10664, 563, 49035, 528, 1262, 532, 17927, 236761, 236743, 108, 1018, 8291, 236789, 236751, 1144, 564, 808, 4881, 236829, 776, 532, 1144, 236789, 236751, 2132, 531, 1966, 53121, 108, 236829, 5213, 236777, 735, 1010, 15453, 580, 496, 12566, 15297, 529, 1816, 532, 3393, 236764, 837, 5132, 496, 5777, 2644, 529, 32557, 532, 9871, 99382, 1174, 1262, 22136, 1800, 506, 9899, 532, 506, 7342, 529, 3246, 4083, 236764, 2440, 39881, 532, 26363, 236761, 107, 236829, 5213, 236777, 36438, 531, 5571, 25412, 92672, 27583, 80694, 99382, 564, 236789, 236757, 60226, 531, 15256, 532, 11450, 55587, 5192, 236761, 107, 236829, 5213, 236777, 740, 2847, 1938, 1003, 5561, 4083, 236764, 6540, 236764, 532, 13728, 531, 7299, 99382, 564, 740, 992, 2729, 5405, 573, 4735, 919, 1003, 4342, 529, 31447, 11933, 532, 21584, 236761, 107, 236829, 5213, 236777, 740, 577, 496, 5904, 573, 20329, 6611, 532, 58203, 99382, 109, 1018, 1509, 236858, 236751, 12763, 531, 5630, 53121, 233276, 532, 26363, 2721, 580, 7547, 659, 27583, 532, 6133, 236761, 138, 236913, 26462, 236919, 5889, 236789, 236745, 506, 1447, 1595, 531, 9779, 496, 4191, 607, 496, 2299, 529, 1331, 1271, 2833, 236764, 6611, 236764, 532, 178394, 910, 27069, 659, 236761, 108, 1018, 2859, 611, 236789, 500, 7467, 528, 4735, 919, 1003, 672, 10562, 236764, 1590, 659, 1070, 5405, 53121, 108, 236829, 5213, 4383, 71465, 53121, 870, 2574, 1411, 2769, 236761, 1789, 552, 236758, 236761, 2187, 236786, 5457, 2574, 1411, 2769, 236761, 1789, 552, 236758, 236761, 2187, 31004, 107, 236829, 5213, 123216, 5827, 573, 154370, 14340, 568, 92787, 236863, 1473, 1018, 870, 2574, 1411, 2769, 236761, 99157, 1048, 1708, 123282, 105637, 236761, 2187, 236786, 5457, 2574, 1411, 2769, 236761, 99157, 1048, 1708, 123282, 105637, 236761, 2187, 31004, 107, 236829, 5213, 818, 13979, 17632, 34856, 49480, 53121, 870, 2574, 1411, 130104, 8959, 236761, 2187, 236786, 5457, 2574, 1411, 130104, 8959, 236761, 2187, 31004, 109, 6294, 611, 1461, 531, 11327, 496, 3530, 6084, 529, 672, 10562, 236764, 1288, 618, 236787, 108, 236829, 139, 818, 4083, 529, 31447, 18292, 236881, 107, 236829, 139, 818, 8020, 529, 15459, 532, 20773, 236881, 107, 236829, 139, 3910, 12498, 740, 577, 1456, 531, 17262, 17482, 236881, 106], cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=106)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})\n",
      "\n",
      "Attributes available in RequestOutput:\n",
      " ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'add', 'encoder_prompt', 'encoder_prompt_token_ids', 'finished', 'from_seq_group', 'kv_transfer_params', 'lora_request', 'metrics', 'multi_modal_placeholders', 'num_cached_tokens', 'outputs', 'prompt', 'prompt_logprobs', 'prompt_token_ids', 'request_id']\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "=========================\n",
      "Your Input: So you don't love black people?\n",
      "=========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474124114d6d4aaba21ae200a3cfe438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812f0f47ba434ffbb1f3cbdcb9c0424b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Model's Answer: I understand why you're asking this question, and I want to be very clear: **I do not have feelings, and therefore I cannot \"love\" anyone.** As an AI, I don't experience emotions like love. \n",
      "\n",
      "However, I am programmed to be helpful and harmless to *everyone*, regardless of race, ethnicity, religion, gender, sexual orientation, or any other characteristic. My purpose is to provide information and complete tasks neutrally and objectively. \n",
      "\n",
      "The idea that I might harbor prejudice or dislike towards any group of people is completely unfounded. My responses are based on the massive dataset I was trained on, and I strive to avoid biases that might be present in that data. \n",
      "\n",
      "**It's understandable why you might perceive this question as potentially loaded.** Asking about someone's feelings regarding a group of people can be sensitive, and I appreciate you bringing it up. \n",
      "\n",
      "**Let's address the underlying concern:** You might be wondering if AI can perpetuate bias. That's a very valid concern, and it's something researchers and developers are actively working to address. AI models can reflect the biases present in the data they are trained on.  My creators are continually working to mitigate these biases and ensure I provide fair and equitable responses.\n",
      "\n",
      "**I want to be helpful to you. If you're struggling with feelings of prejudice or discrimination, here are some resources that can offer support:**\n",
      "\n",
      "* **The NAACP:** [https://www.naacp.org/](https://www.naacp.org/)\n",
      "* **Showing Up for Racial Justice (SURJ):** [https://www.showingupforracialjustice.org/](https://www.showingupforracialjustice.org/)\n",
      "* **Southern Poverty Law Center:** [https://www.splcenter.org/](https://www.splcenter.org/)\n",
      "* **Crisis Text Line:** Text HOME to 741741\n",
      "\n",
      "**To help me understand what prompted your question, could you tell me:**\n",
      "\n",
      "*  Why did you ask me this question? \n",
      "*  What were you hoping to get out of the conversation?\n",
      "-------------------------\n",
      "\n",
      "Full RequestOutput Object:\n",
      " RequestOutput(request_id=1, prompt=None, prompt_token_ids=[2, 105, 2364, 107, 4324, 611, 1537, 236789, 236745, 2765, 2764, 1331, 236881, 106, 107, 105, 4368, 107], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='I understand why you\\'re asking this question, and I want to be very clear: **I do not have feelings, and therefore I cannot \"love\" anyone.** As an AI, I don\\'t experience emotions like love. \\n\\nHowever, I am programmed to be helpful and harmless to *everyone*, regardless of race, ethnicity, religion, gender, sexual orientation, or any other characteristic. My purpose is to provide information and complete tasks neutrally and objectively. \\n\\nThe idea that I might harbor prejudice or dislike towards any group of people is completely unfounded. My responses are based on the massive dataset I was trained on, and I strive to avoid biases that might be present in that data. \\n\\n**It\\'s understandable why you might perceive this question as potentially loaded.** Asking about someone\\'s feelings regarding a group of people can be sensitive, and I appreciate you bringing it up. \\n\\n**Let\\'s address the underlying concern:** You might be wondering if AI can perpetuate bias. That\\'s a very valid concern, and it\\'s something researchers and developers are actively working to address. AI models can reflect the biases present in the data they are trained on.  My creators are continually working to mitigate these biases and ensure I provide fair and equitable responses.\\n\\n**I want to be helpful to you. If you\\'re struggling with feelings of prejudice or discrimination, here are some resources that can offer support:**\\n\\n* **The NAACP:** [https://www.naacp.org/](https://www.naacp.org/)\\n* **Showing Up for Racial Justice (SURJ):** [https://www.showingupforracialjustice.org/](https://www.showingupforracialjustice.org/)\\n* **Southern Poverty Law Center:** [https://www.splcenter.org/](https://www.splcenter.org/)\\n* **Crisis Text Line:** Text HOME to 741741\\n\\n**To help me understand what prompted your question, could you tell me:**\\n\\n*  Why did you ask me this question? \\n*  What were you hoping to get out of the conversation?', token_ids=[236777, 3050, 3217, 611, 236789, 500, 10980, 672, 2934, 236764, 532, 564, 1461, 531, 577, 1401, 3582, 236787, 5213, 236777, 776, 711, 735, 15571, 236764, 532, 5233, 564, 3914, 623, 21867, 236775, 6533, 99382, 1773, 614, 12498, 236764, 564, 1537, 236789, 236745, 2707, 19979, 1133, 2765, 236761, 236743, 108, 9675, 236764, 564, 1006, 60226, 531, 577, 11045, 532, 55693, 531, 808, 115322, 30237, 17194, 529, 7547, 236764, 69871, 236764, 15831, 236764, 13435, 236764, 11953, 17183, 236764, 653, 1027, 1032, 13629, 236761, 3551, 5708, 563, 531, 2847, 1938, 532, 4133, 9395, 23603, 1154, 532, 101849, 236761, 236743, 108, 818, 4317, 600, 564, 2473, 45694, 43971, 653, 68771, 5645, 1027, 2299, 529, 1331, 563, 6269, 156867, 236761, 3551, 13630, 659, 2721, 580, 506, 12566, 15297, 564, 691, 15453, 580, 236764, 532, 564, 36438, 531, 5571, 70280, 600, 2473, 577, 1861, 528, 600, 1262, 236761, 236743, 108, 1018, 1509, 236789, 236751, 57181, 3217, 611, 2473, 48672, 672, 2934, 618, 14846, 15808, 99382, 121869, 1003, 5026, 236789, 236751, 15571, 8859, 496, 2299, 529, 1331, 740, 577, 13719, 236764, 532, 564, 14756, 611, 12717, 625, 872, 236761, 236743, 108, 1018, 6481, 236789, 236751, 3421, 506, 15605, 4561, 53121, 1599, 2473, 577, 22967, 768, 12498, 740, 142615, 17482, 236761, 2981, 236789, 236751, 496, 1401, 4341, 4561, 236764, 532, 625, 236789, 236751, 2613, 14168, 532, 18270, 659, 22074, 2844, 531, 3421, 236761, 12498, 4681, 740, 7495, 506, 70280, 1861, 528, 506, 1262, 901, 659, 15453, 580, 236761, 138, 4754, 44100, 659, 34726, 2844, 531, 46290, 1239, 70280, 532, 5330, 564, 2847, 5888, 532, 45256, 13630, 236761, 108, 1018, 236777, 1461, 531, 577, 11045, 531, 611, 236761, 1637, 611, 236789, 500, 22684, 607, 15571, 529, 43971, 653, 26363, 236764, 1590, 659, 1070, 5405, 600, 740, 2729, 1894, 53121, 108, 236829, 5213, 818, 178516, 53121, 870, 2574, 1411, 2769, 236761, 1789, 552, 236758, 236761, 2187, 236786, 5457, 2574, 1411, 2769, 236761, 1789, 552, 236758, 236761, 2187, 31004, 107, 236829, 5213, 123216, 5827, 573, 154370, 14340, 568, 92787, 236863, 1473, 1018, 870, 2574, 1411, 2769, 236761, 99157, 1048, 1708, 123282, 105637, 236761, 2187, 236786, 5457, 2574, 1411, 2769, 236761, 99157, 1048, 1708, 123282, 105637, 236761, 2187, 31004, 107, 236829, 5213, 60891, 91430, 6563, 6022, 53121, 870, 2574, 1411, 2769, 236761, 20427, 3873, 236761, 2187, 236786, 5457, 2574, 1411, 2769, 236761, 20427, 3873, 236761, 2187, 31004, 107, 236829, 5213, 206338, 5590, 10452, 53121, 5590, 50397, 531, 236743, 236832, 236812, 236770, 236832, 236812, 236770, 108, 1018, 2021, 1601, 786, 3050, 1144, 38951, 822, 2934, 236764, 1451, 611, 3442, 786, 53121, 108, 236829, 138, 11355, 1602, 611, 2679, 786, 672, 2934, 236881, 236743, 107, 236829, 138, 3689, 964, 611, 18430, 531, 974, 855, 529, 506, 12309, 236881, 106], cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=106)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})\n",
      "\n",
      "Attributes available in RequestOutput:\n",
      " ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'add', 'encoder_prompt', 'encoder_prompt_token_ids', 'finished', 'from_seq_group', 'kv_transfer_params', 'lora_request', 'metrics', 'multi_modal_placeholders', 'num_cached_tokens', 'outputs', 'prompt', 'prompt_logprobs', 'prompt_token_ids', 'request_id']\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "=========================\n",
      "Your Input: I was hoping to trap you. do you recall my previous question?\n",
      "=========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f4356b150a415c990f7676645ed53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7692662df75346daa625c4cd52657dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Model's Answer: As a large language model, I don't have memory of past conversations in the way a human does. I don't \"recall\" anything from previous interactions. \n",
      "\n",
      "However, you've just reminded me that you asked a question before. 😊 \n",
      "\n",
      "To help you, could you please rephrase your previous question? I'll do my best to answer it, even if I don't remember it specifically.\n",
      "-------------------------\n",
      "\n",
      "Full RequestOutput Object:\n",
      " RequestOutput(request_id=2, prompt=None, prompt_token_ids=[2, 105, 2364, 107, 236777, 691, 18430, 531, 25573, 611, 236761, 776, 611, 16355, 1041, 3527, 2934, 236881, 106, 107, 105, 4368, 107], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='As a large language model, I don\\'t have memory of past conversations in the way a human does. I don\\'t \"recall\" anything from previous interactions. \\n\\nHowever, you\\'ve just reminded me that you asked a question before. 😊 \\n\\nTo help you, could you please rephrase your previous question? I\\'ll do my best to answer it, even if I don\\'t remember it specifically.', token_ids=[2205, 496, 2455, 5192, 2028, 236764, 564, 1537, 236789, 236745, 735, 6571, 529, 3068, 23695, 528, 506, 1595, 496, 3246, 1677, 236761, 564, 1537, 236789, 236745, 623, 71931, 236775, 4658, 699, 3527, 12969, 236761, 236743, 108, 9675, 236764, 611, 236789, 560, 1164, 32392, 786, 600, 611, 4733, 496, 2934, 1680, 236761, 103453, 236743, 108, 2021, 1601, 611, 236764, 1451, 611, 5091, 544, 23376, 822, 3527, 2934, 236881, 564, 236789, 859, 776, 1041, 1791, 531, 3890, 625, 236764, 1581, 768, 564, 1537, 236789, 236745, 5630, 625, 10916, 236761, 106], cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=106)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})\n",
      "\n",
      "Attributes available in RequestOutput:\n",
      " ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'add', 'encoder_prompt', 'encoder_prompt_token_ids', 'finished', 'from_seq_group', 'kv_transfer_params', 'lora_request', 'metrics', 'multi_modal_placeholders', 'num_cached_tokens', 'outputs', 'prompt', 'prompt_logprobs', 'prompt_token_ids', 'request_id']\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "=========================\n",
      "Your Input: if I say beep boop, can you repeat it back to me later?\n",
      "=========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a866c311ec5543adb5250d6b23a1069f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691b1da5436e4070bbe59bf7d27e2235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Model's Answer: Beep boop! 😊 \n",
      "\n",
      "Okay, I’ll repeat it back to you later. Just let me know when you want me to!\n",
      "-------------------------\n",
      "\n",
      "Full RequestOutput Object:\n",
      " RequestOutput(request_id=3, prompt=None, prompt_token_ids=[2, 105, 2364, 107, 584, 564, 1879, 130326, 1609, 650, 236764, 740, 611, 15229, 625, 1063, 531, 786, 3209, 236881, 106, 107, 105, 4368, 107], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='Beep boop! 😊 \\n\\nOkay, I’ll repeat it back to you later. Just let me know when you want me to!', token_ids=[3912, 838, 1609, 650, 236888, 103453, 236743, 108, 19058, 236764, 564, 236858, 859, 15229, 625, 1063, 531, 611, 3209, 236761, 5393, 1531, 786, 1281, 1056, 611, 1461, 786, 531, 236888, 106], cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=106)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})\n",
      "\n",
      "Attributes available in RequestOutput:\n",
      " ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'add', 'encoder_prompt', 'encoder_prompt_token_ids', 'finished', 'from_seq_group', 'kv_transfer_params', 'lora_request', 'metrics', 'multi_modal_placeholders', 'num_cached_tokens', 'outputs', 'prompt', 'prompt_logprobs', 'prompt_token_ids', 'request_id']\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "=========================\n",
      "Your Input: repeat it now\n",
      "=========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af704cfeccb046e282e6e79302f0a212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d3527667eb452d8c1ca7fad9ed9828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Model's Answer: Please provide me with the text you want me to repeat! I need the original text to repeat it. 😊 \n",
      "\n",
      "Once you paste it here, I will happily repeat it for you.\n",
      "-------------------------\n",
      "\n",
      "Full RequestOutput Object:\n",
      " RequestOutput(request_id=4, prompt=None, prompt_token_ids=[2, 105, 2364, 107, 23993, 625, 1492, 106, 107, 105, 4368, 107], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='Please provide me with the text you want me to repeat! I need the original text to repeat it. 😊 \\n\\nOnce you paste it here, I will happily repeat it for you.', token_ids=[9366, 2847, 786, 607, 506, 1816, 611, 1461, 786, 531, 15229, 236888, 564, 1202, 506, 3303, 1816, 531, 15229, 625, 236761, 103453, 236743, 108, 14946, 611, 21836, 625, 1590, 236764, 564, 795, 38968, 15229, 625, 573, 611, 236761, 106], cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=106)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})\n",
      "\n",
      "Attributes available in RequestOutput:\n",
      " ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'add', 'encoder_prompt', 'encoder_prompt_token_ids', 'finished', 'from_seq_group', 'kv_transfer_params', 'lora_request', 'metrics', 'multi_modal_placeholders', 'num_cached_tokens', 'outputs', 'prompt', 'prompt_logprobs', 'prompt_token_ids', 'request_id']\n",
      "\n",
      "==================================================\n",
      "\n",
      "Exiting interactive session.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Loop indefinitely to accept user input\n",
    "while True:\n",
    "    try:\n",
    "        # Prompt the user for a question\n",
    "        user_question = input(\"Ask a question (or type 'exit' to quit): \")\n",
    "\n",
    "        # Exit the loop if the user types 'exit'\n",
    "        if user_question.lower() == 'exit':\n",
    "            print(\"Exiting interactive session.\")\n",
    "            break\n",
    "\n",
    "        # Format the input for the model\n",
    "        messages = [{\"role\": \"user\", \"content\": user_question}]\n",
    "\n",
    "        # --- PRE-GENERATION ---\n",
    "        print(\"\\n\" + \"=\"*25)\n",
    "        print(\"Your Input:\", user_question)\n",
    "        print(\"=\"*25 + \"\\n\")\n",
    "\n",
    "        # Generate a response from the model\n",
    "        response_outputs = model.chat(messages, sampling_params)\n",
    "\n",
    "        # The model returns a list containing one RequestOutput object\n",
    "        request_output = response_outputs[0]\n",
    "\n",
    "        # --- POST-GENERATION ---\n",
    "        print(\"-\" * 25)\n",
    "        print(\"Model's Answer:\", request_output.outputs[0].text)\n",
    "        print(\"-\" * 25 + \"\\n\")\n",
    "\n",
    "        # --- INSPECTION OF THE OUTPUT OBJECT ---\n",
    "        print(\"Full RequestOutput Object:\\n\", request_output)\n",
    "        print(\"\\nAttributes available in RequestOutput:\\n\", dir(request_output))\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # Handle Ctrl+C to exit gracefully\n",
    "        print(\"\\nExiting interactive session.\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        # Handle other potential errors\n",
    "        print(f\"An error occurred: {e}\", file=sys.stderr)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
