{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcf886b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install --upgrade pip -q\n",
    "# !pip install uv -qU\n",
    "# !uv pip install \"numpy<2.3\" \"transformers<=4.53.2\" datasets tensorboard openai hf_transfer accelerate pillow scikit-learn pymupdf google.generativeai flashinfer-python huggingface_hub vllm ipywidgets -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bf92976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sw/arch/RHEL9/EB_production/2024/software/CUDA/12.6.0/bin/nvcc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# The path where 'which nvcc' found the compiler in your terminal\n",
    "cuda_path = '/sw/arch/RHEL9/EB_production/2024/software/CUDA/12.6.0/bin'\n",
    "\n",
    "# Prepend the CUDA path to the environment variable\n",
    "os.environ['PATH'] = f\"{cuda_path}:{os.environ['PATH']}\"\n",
    "\n",
    "# Verify the change\n",
    "!which nvcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "692e4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !uv pip install datasets tensorboard openai hf_transfer accelerate pillow -qU\n",
    "# !uv pip install scikit-learn pymupdf -qU\n",
    "# !uv pip install google.generativeai  -qU # for gemini\n",
    "# !uv pip install flashinfer-python  -qU # helpful for speeding up vllm\n",
    "# !uv pip install huggingface_hub -qU\n",
    "# !uv pip install vllm -qU\n",
    "# !uv pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30ab1be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HF_HOME'] = '/scratch-shared/amark/huggingface_cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e47826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CPU cores available: 64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Get the total number of CPU cores available in your Colab instance\n",
    "total_cores = os.cpu_count()\n",
    "print(f\"Total CPU cores available: {total_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99e50c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder, login\n",
    "\n",
    "# Check if a token is already saved\n",
    "if HfFolder.get_token() is None:\n",
    "    login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e06f6abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_slug = \"google/gemma-3-27b-it\"\n",
    "# model_slug = \"Ardjano/gemma3-27b-finetune1-merged\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "861dfe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"../data/humanandllm.csv\")\n",
    "df = df[['Context', 'Response']]\n",
    "\n",
    "trainset, testset = train_test_split(df, test_size=0.057, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60c9c1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have no real friends. I have a girlfriend who irritates me but loves me to death. I push her away and pushes me away. Weâ€™re going through a breakup, and I have nobody.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(testset.iloc[1].Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d93d25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: bfloat16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = \"bfloat16\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  if \"T4\" in torch.cuda.get_device_name(0):\n",
    "    dtype = \"float16\"\n",
    "else:\n",
    "  dtype = \"float32\"\n",
    "\n",
    "print(f\"dtype: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99822fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_slug)\n",
    "# print(f\"Tokenizer type: {type(tokenizer)}\")\n",
    "# print(f\"Tokenizer model name: {tokenizer.name_or_path}\")\n",
    "\n",
    "# The original model with the correct tokenizer and chat template\n",
    "tokenizer_source = \"google/gemma-3-27b-it\"\n",
    "\n",
    "# Load the tokenizer from the original source\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "495a1a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_generation_prompt(context_text, tokenizer_obj):\n",
    "    \"\"\"\n",
    "    Formats a single context (user's message) into the chat template for model generation.\n",
    "    \"\"\"\n",
    "    # Create the conversation structure directly with the provided context_text\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": context_text},\n",
    "    ]\n",
    "    # For generation, add_generation_prompt should be True\n",
    "    formatted_prompt = tokenizer_obj.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return formatted_prompt\n",
    "\n",
    "prompts = []\n",
    "original_prompts = []\n",
    "true_responses = []\n",
    "\n",
    "for i, row in testset.iterrows():\n",
    "    context_text = row['Context']\n",
    "    true_response_text = row['Response']\n",
    "\n",
    "    formatted_input_string = format_generation_prompt(context_text, tokenizer)\n",
    "\n",
    "    prompts.append(formatted_input_string)\n",
    "    original_prompts.append(context_text)\n",
    "    true_responses.append(true_response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb202bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_slug,\n",
    "    use_fast=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9e6b4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 14:44:20 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529c401ce18f474ca7e8139f9731c615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/972 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b984d5fefa3e4f3eb5208cd1abc454e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 14:44:34 [config.py:823] This model supports multiple tasks: {'score', 'classify', 'generate', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 09-26 14:44:34 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=16384.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9751d7b0f2b940cd814f622b502e4ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-26 14:44:37 [utils.py:2597] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "WARNING 09-26 14:44:38 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\n",
      "INFO 09-26 14:44:41 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 09-26 14:44:44 [core.py:455] Waiting for init message from front-end.\n",
      "INFO 09-26 14:44:44 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='google/gemma-3-27b-it', speculative_config=None, tokenizer='google/gemma-3-27b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/gemma-3-27b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 09-26 14:44:45 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x1530ddced390>\n",
      "INFO 09-26 14:44:46 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 14:44:54 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 09-26 14:44:54 [gpu_model_runner.py:1595] Starting to load model google/gemma-3-27b-it...\n",
      "INFO 09-26 14:44:54 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "INFO 09-26 14:44:54 [cuda.py:252] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 14:44:55 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "INFO 09-26 14:51:14 [weight_utils.py:308] Time spent downloading weights for google/gemma-3-27b-it: 379.500220 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   8% Completed | 1/12 [00:07<01:22,  7.54s/it]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 2/12 [00:13<01:08,  6.83s/it]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 3/12 [00:19<00:55,  6.13s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 4/12 [00:24<00:47,  5.94s/it]\n",
      "Loading safetensors checkpoint shards:  42% Completed | 5/12 [00:30<00:40,  5.77s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 6/12 [00:31<00:26,  4.38s/it]\n",
      "Loading safetensors checkpoint shards:  58% Completed | 7/12 [00:37<00:23,  4.75s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 8/12 [00:43<00:20,  5.12s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 9/12 [00:48<00:15,  5.23s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 10/12 [00:54<00:10,  5.49s/it]\n",
      "Loading safetensors checkpoint shards:  92% Completed | 11/12 [01:00<00:05,  5.66s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 12/12 [01:06<00:00,  5.60s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 12/12 [01:06<00:00,  5.54s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 14:52:21 [default_loader.py:272] Loading weights took 66.49 seconds\n",
      "INFO 09-26 14:52:21 [gpu_model_runner.py:1624] Model loading took 51.4479 GiB and 446.819583 seconds\n",
      "INFO 09-26 14:52:22 [gpu_model_runner.py:1978] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 64 image items of the maximum feature size.\n",
      "INFO 09-26 14:52:40 [backends.py:462] Using cache directory: /home/amark/.cache/vllm/torch_compile_cache/b7c40999f8/rank_0_0 for vLLM's torch.compile\n",
      "INFO 09-26 14:52:40 [backends.py:472] Dynamo bytecode transform time: 15.83 s\n",
      "INFO 09-26 14:52:45 [backends.py:161] Cache the graph of shape None for later use\n",
      "INFO 09-26 14:53:44 [backends.py:173] Compiling a graph for general shape takes 62.68 s\n",
      "INFO 09-26 14:54:40 [monitor.py:34] torch.compile takes 78.51 s in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 14:54:42 [gpu_worker.py:227] Available KV cache memory: 25.10 GiB\n",
      "WARNING 09-26 14:54:42 [kv_cache_utils.py:830] Add 8 padding layers, may waste at most 15.38% KV cache memory\n",
      "INFO 09-26 14:54:42 [kv_cache_utils.py:870] GPU KV cache size: 46,992 tokens\n",
      "INFO 09-26 14:54:42 [kv_cache_utils.py:874] Maximum concurrency for 8,192 tokens per request: 5.73x\n",
      "INFO 09-26 14:55:24 [gpu_model_runner.py:2048] Graph capturing finished in 42 secs, took 0.81 GiB\n",
      "INFO 09-26 14:55:24 [core.py:171] init engine (profile, create kv cache, warmup model) took 182.86 seconds\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=1024,\n",
    "    top_k=64,\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    "    min_p=0.0\n",
    ")\n",
    "\n",
    "model = LLM(\n",
    "    model=model_slug,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    dtype=dtype,\n",
    "    max_model_len=8192\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9679ceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip freeze --color never > eval_packages.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c3ea21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218e604164e247ca8cc768678658c84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e1926796304f53983049825fe5dc4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6a8851ba1649fa9e9e95e78c9f7fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing outputs:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 200 generated responses.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "outputs = model.generate(prompts, sampling_params)\n",
    "\n",
    "generated_responses_data = []\n",
    "\n",
    "# Process the outputs\n",
    "for i, output in tqdm(enumerate(outputs), total=len(outputs), desc=\"Processing outputs\"):\n",
    "    generated_text = output.outputs[0].text.strip()\n",
    "\n",
    "    generated_responses_data.append({\n",
    "        \"Context\": original_prompts[i],\n",
    "        \"True_Response\": true_responses[i],\n",
    "        \"Generated_Response\": generated_text\n",
    "    })\n",
    "\n",
    "print(f\"Finished processing {len(generated_responses_data)} generated responses.\")\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "responses_df = pd.DataFrame(generated_responses_data)\n",
    "output_csv_path = \"vllm_model_test_responses_base_gemma3-27b.csv\"\n",
    "responses_df.to_csv(output_csv_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
