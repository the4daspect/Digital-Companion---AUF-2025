{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcf886b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install --upgrade pip -q\n",
    "# !pip install uv -qU\n",
    "# !uv pip install \"numpy<2.3\" \"transformers<=4.53.2\" datasets tensorboard openai hf_transfer accelerate pillow scikit-learn pymupdf google.generativeai flashinfer-python huggingface_hub vllm ipywidgets -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "692e4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !uv pip install datasets tensorboard openai hf_transfer accelerate pillow -qU\n",
    "# !uv pip install scikit-learn pymupdf -qU\n",
    "# !uv pip install google.generativeai  -qU # for gemini\n",
    "# !uv pip install flashinfer-python  -qU # helpful for speeding up vllm\n",
    "# !uv pip install huggingface_hub -qU\n",
    "# !uv pip install vllm -qU\n",
    "# !uv pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32e47826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CPU cores available: 64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the total number of CPU cores available in your Colab instance\n",
    "total_cores = os.cpu_count()\n",
    "print(f\"Total CPU cores available: {total_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99e50c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder, login\n",
    "\n",
    "# Check if a token is already saved\n",
    "if HfFolder.get_token() is None:\n",
    "    login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e06f6abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_slug = \"google/gemma-3-4b-it\"\n",
    "model_slug = \"google/gemma-3-27b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "861dfe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"../data/humanandllm.csv\")\n",
    "df = df[['Context', 'Response']]\n",
    "\n",
    "trainset, testset = train_test_split(df, test_size=0.057, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60c9c1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have no real friends. I have a girlfriend who irritates me but loves me to death. I push her away and pushes me away. Weâ€™re going through a breakup, and I have nobody.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(testset.iloc[1].Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d93d25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: bfloat16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = \"bfloat16\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  if \"T4\" in torch.cuda.get_device_name(0):\n",
    "    dtype = \"float16\"\n",
    "else:\n",
    "  dtype = \"float32\"\n",
    "\n",
    "print(f\"dtype: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99822fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer type: <class 'transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast'>\n",
      "Tokenizer model name: google/gemma-3-27b-it\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_slug)\n",
    "print(f\"Tokenizer type: {type(tokenizer)}\")\n",
    "print(f\"Tokenizer model name: {tokenizer.name_or_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "495a1a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_generation_prompt(context_text, tokenizer_obj):\n",
    "    \"\"\"\n",
    "    Formats a single context (user's message) into the chat template for model generation.\n",
    "    \"\"\"\n",
    "    # Create the conversation structure directly with the provided context_text\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": context_text},\n",
    "    ]\n",
    "    # For generation, add_generation_prompt should be True\n",
    "    formatted_prompt = tokenizer_obj.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return formatted_prompt\n",
    "\n",
    "prompts = []\n",
    "original_prompts = []\n",
    "true_responses = []\n",
    "\n",
    "for i, row in testset.iterrows():\n",
    "    context_text = row['Context']\n",
    "    true_response_text = row['Response']\n",
    "\n",
    "    formatted_input_string = format_generation_prompt(context_text, tokenizer)\n",
    "\n",
    "    prompts.append(formatted_input_string)\n",
    "    original_prompts.append(context_text)\n",
    "    true_responses.append(true_response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb202bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_slug,\n",
    "    use_fast=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9e6b4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 03:59:31 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 09-26 03:59:43 [config.py:823] This model supports multiple tasks: {'score', 'reward', 'generate', 'classify', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 09-26 03:59:43 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=16384.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-26 03:59:45 [utils.py:2597] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "WARNING 09-26 03:59:47 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\n",
      "INFO 09-26 03:59:50 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 09-26 03:59:53 [core.py:455] Waiting for init message from front-end.\n",
      "INFO 09-26 03:59:53 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='google/gemma-3-27b-it', speculative_config=None, tokenizer='google/gemma-3-27b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/gemma-3-27b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 09-26 03:59:54 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x149c1ba755d0>\n",
      "INFO 09-26 03:59:54 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 04:00:02 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 09-26 04:00:02 [gpu_model_runner.py:1595] Starting to load model google/gemma-3-27b-it...\n",
      "INFO 09-26 04:00:02 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "INFO 09-26 04:00:02 [cuda.py:252] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 04:00:03 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "ERROR 09-26 04:00:38 [core.py:515] EngineCore failed to start.\n",
      "ERROR 09-26 04:00:38 [core.py:515] Traceback (most recent call last):\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 506, in run_engine_core\n",
      "ERROR 09-26 04:00:38 [core.py:515]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 09-26 04:00:38 [core.py:515]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 390, in __init__\n",
      "ERROR 09-26 04:00:38 [core.py:515]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 76, in __init__\n",
      "ERROR 09-26 04:00:38 [core.py:515]     self.model_executor = executor_class(vllm_config)\n",
      "ERROR 09-26 04:00:38 [core.py:515]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/executor/executor_base.py\", line 53, in __init__\n",
      "ERROR 09-26 04:00:38 [core.py:515]     self._init_executor()\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 48, in _init_executor\n",
      "ERROR 09-26 04:00:38 [core.py:515]     self.collective_rpc(\"load_model\")\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\n",
      "ERROR 09-26 04:00:38 [core.py:515]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 09-26 04:00:38 [core.py:515]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/utils.py\", line 2671, in run_method\n",
      "ERROR 09-26 04:00:38 [core.py:515]     return func(*args, **kwargs)\n",
      "ERROR 09-26 04:00:38 [core.py:515]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 180, in load_model\n",
      "ERROR 09-26 04:00:38 [core.py:515]     self.model_runner.load_model()\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1601, in load_model\n",
      "ERROR 09-26 04:00:38 [core.py:515]     self.model = model_loader.load_model(\n",
      "ERROR 09-26 04:00:38 [core.py:515]                  ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 41, in load_model\n",
      "ERROR 09-26 04:00:38 [core.py:515]     self.load_weights(model, model_config)\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 269, in load_weights\n",
      "ERROR 09-26 04:00:38 [core.py:515]     loaded_weights = model.load_weights(\n",
      "ERROR 09-26 04:00:38 [core.py:515]                      ^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/models/gemma3_mm.py\", line 700, in load_weights\n",
      "ERROR 09-26 04:00:38 [core.py:515]     return loader.load_weights(weights)\n",
      "ERROR 09-26 04:00:38 [core.py:515]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 278, in load_weights\n",
      "ERROR 09-26 04:00:38 [core.py:515]     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n",
      "ERROR 09-26 04:00:38 [core.py:515]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 227, in _load_module\n",
      "ERROR 09-26 04:00:38 [core.py:515]     for child_prefix, child_weights in self._groupby_prefix(weights):\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 116, in _groupby_prefix\n",
      "ERROR 09-26 04:00:38 [core.py:515]     for prefix, group in itertools.groupby(weights_by_parts,\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 113, in <genexpr>\n",
      "ERROR 09-26 04:00:38 [core.py:515]     weights_by_parts = ((weight_name.split(\".\", 1), weight_data)\n",
      "ERROR 09-26 04:00:38 [core.py:515]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 275, in <genexpr>\n",
      "ERROR 09-26 04:00:38 [core.py:515]     weights = ((name, weight) for name, weight in weights\n",
      "ERROR 09-26 04:00:38 [core.py:515]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 251, in get_all_weights\n",
      "ERROR 09-26 04:00:38 [core.py:515]     yield from self._get_weights_iterator(primary_weights)\n",
      "ERROR 09-26 04:00:38 [core.py:515]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 178, in _get_weights_iterator\n",
      "ERROR 09-26 04:00:38 [core.py:515]     hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(\n",
      "ERROR 09-26 04:00:38 [core.py:515]                                                    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 131, in _prepare_weights\n",
      "ERROR 09-26 04:00:38 [core.py:515]     hf_folder = download_weights_from_hf(\n",
      "ERROR 09-26 04:00:38 [core.py:515]                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/model_loader/weight_utils.py\", line 297, in download_weights_from_hf\n",
      "ERROR 09-26 04:00:38 [core.py:515]     hf_folder = snapshot_download(\n",
      "ERROR 09-26 04:00:38 [core.py:515]                 ^^^^^^^^^^^^^^^^^^\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "ERROR 09-26 04:00:38 [core.py:515]     return fn(*args, **kwargs)\n",
      "ERROR 09-26 04:00:38 [core.py:515]            ^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/huggingface_hub/_snapshot_download.py\", line 330, in snapshot_download\n",
      "ERROR 09-26 04:00:38 [core.py:515]     _inner_hf_hub_download(file)\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/huggingface_hub/_snapshot_download.py\", line 306, in _inner_hf_hub_download\n",
      "ERROR 09-26 04:00:38 [core.py:515]     return hf_hub_download(\n",
      "ERROR 09-26 04:00:38 [core.py:515]            ^^^^^^^^^^^^^^^^\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "ERROR 09-26 04:00:38 [core.py:515]     return fn(*args, **kwargs)\n",
      "ERROR 09-26 04:00:38 [core.py:515]            ^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1010, in hf_hub_download\n",
      "ERROR 09-26 04:00:38 [core.py:515]     return _hf_hub_download_to_cache_dir(\n",
      "ERROR 09-26 04:00:38 [core.py:515]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1171, in _hf_hub_download_to_cache_dir\n",
      "ERROR 09-26 04:00:38 [core.py:515]     _download_to_tmp_and_move(\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1723, in _download_to_tmp_and_move\n",
      "ERROR 09-26 04:00:38 [core.py:515]     xet_get(\n",
      "ERROR 09-26 04:00:38 [core.py:515]   File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 629, in xet_get\n",
      "ERROR 09-26 04:00:38 [core.py:515]     download_files(\n",
      "ERROR 09-26 04:00:38 [core.py:515] RuntimeError: Data processing error: CAS service error : IO Error: Disk quota exceeded (os error 122)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process EngineCore_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 519, in run_engine_core\n",
      "    raise e\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 506, in run_engine_core\n",
      "    engine_core = EngineCoreProc(*args, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 390, in __init__\n",
      "    super().__init__(vllm_config, executor_class, log_stats,\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 76, in __init__\n",
      "    self.model_executor = executor_class(vllm_config)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/executor/executor_base.py\", line 53, in __init__\n",
      "    self._init_executor()\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 48, in _init_executor\n",
      "    self.collective_rpc(\"load_model\")\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\n",
      "    answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/utils.py\", line 2671, in run_method\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 180, in load_model\n",
      "    self.model_runner.load_model()\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1601, in load_model\n",
      "    self.model = model_loader.load_model(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 41, in load_model\n",
      "    self.load_weights(model, model_config)\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 269, in load_weights\n",
      "    loaded_weights = model.load_weights(\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/models/gemma3_mm.py\", line 700, in load_weights\n",
      "    return loader.load_weights(weights)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 278, in load_weights\n",
      "    autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 227, in _load_module\n",
      "    for child_prefix, child_weights in self._groupby_prefix(weights):\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 116, in _groupby_prefix\n",
      "    for prefix, group in itertools.groupby(weights_by_parts,\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 113, in <genexpr>\n",
      "    weights_by_parts = ((weight_name.split(\".\", 1), weight_data)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 275, in <genexpr>\n",
      "    weights = ((name, weight) for name, weight in weights\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 251, in get_all_weights\n",
      "    yield from self._get_weights_iterator(primary_weights)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 178, in _get_weights_iterator\n",
      "    hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(\n",
      "                                                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 131, in _prepare_weights\n",
      "    hf_folder = download_weights_from_hf(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/model_executor/model_loader/weight_utils.py\", line 297, in download_weights_from_hf\n",
      "    hf_folder = snapshot_download(\n",
      "                ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/huggingface_hub/_snapshot_download.py\", line 330, in snapshot_download\n",
      "    _inner_hf_hub_download(file)\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/huggingface_hub/_snapshot_download.py\", line 306, in _inner_hf_hub_download\n",
      "    return hf_hub_download(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1010, in hf_hub_download\n",
      "    return _hf_hub_download_to_cache_dir(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1171, in _hf_hub_download_to_cache_dir\n",
      "    _download_to_tmp_and_move(\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1723, in _download_to_tmp_and_move\n",
      "    xet_get(\n",
      "  File \"/home/amark/.conda/envs/vllm_env/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 629, in xet_get\n",
      "    download_files(\n",
      "RuntimeError: Data processing error: CAS service error : IO Error: Disk quota exceeded (os error 122)\n",
      "[rank0]:[W926 04:00:39.829263660 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM, SamplingParams\n\u001b[32m      6\u001b[39m sampling_params = SamplingParams(\n\u001b[32m      7\u001b[39m     max_tokens=\u001b[32m1024\u001b[39m,\n\u001b[32m      8\u001b[39m     top_k=\u001b[32m64\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     min_p=\u001b[32m0.0\u001b[39m\n\u001b[32m     12\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m model = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_slug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8192\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/entrypoints/llm.py:243\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, task, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m engine_args = EngineArgs(\n\u001b[32m    214\u001b[39m     model=model,\n\u001b[32m    215\u001b[39m     task=task,\n\u001b[32m   (...)\u001b[39m\u001b[32m    239\u001b[39m     **kwargs,\n\u001b[32m    240\u001b[39m )\n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    247\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/engine/llm_engine.py:501\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    498\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv1\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[32m    499\u001b[39m     engine_cls = V1LLMEngine\n\u001b[32m--> \u001b[39m\u001b[32m501\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py:124\u001b[39m, in \u001b[36mLLMEngine.from_vllm_config\u001b[39m\u001b[34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_vllm_config\u001b[39m(\n\u001b[32m    118\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    122\u001b[39m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    123\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLLMEngine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m               \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mExecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m               \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m               \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m               \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m               \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVLLM_ENABLE_V1_MULTIPROCESSING\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py:101\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28mself\u001b[39m.output_processor = OutputProcessor(\u001b[38;5;28mself\u001b[39m.tokenizer,\n\u001b[32m     98\u001b[39m                                         log_stats=\u001b[38;5;28mself\u001b[39m.log_stats)\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiprocess_mode:\n\u001b[32m    110\u001b[39m     \u001b[38;5;66;03m# for v0 compatibility\u001b[39;00m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_executor = \u001b[38;5;28mself\u001b[39m.engine_core.engine_core.model_executor  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:75\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m EngineCoreClient.make_async_mp_client(\n\u001b[32m     72\u001b[39m         vllm_config, executor_class, log_stats)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:558\u001b[39m, in \u001b[36mSyncMPClient.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[32m    557\u001b[39m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m558\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    565\u001b[39m     \u001b[38;5;28mself\u001b[39m.outputs_queue = queue.Queue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n\u001b[32m    567\u001b[39m     \u001b[38;5;66;03m# Ensure that the outputs socket processing thread does not have\u001b[39;00m\n\u001b[32m    568\u001b[39m     \u001b[38;5;66;03m# a ref to the client which prevents gc.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:422\u001b[39m, in \u001b[36mMPClient.__init__\u001b[39m\u001b[34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m.resources.output_socket = make_zmq_socket(\n\u001b[32m    419\u001b[39m     \u001b[38;5;28mself\u001b[39m.ctx, output_address, zmq.PULL)\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m client_addresses \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_engines_direct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mlocal_start_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m                              \u001b[49m\u001b[43moutput_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    426\u001b[39m     coordinator = \u001b[38;5;28mself\u001b[39m.resources.coordinator\n\u001b[32m    427\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m coordinator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:491\u001b[39m, in \u001b[36mMPClient._init_engines_direct\u001b[39m\u001b[34m(self, vllm_config, local_only, local_start_index, input_address, output_address, executor_class, log_stats)\u001b[39m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28mself\u001b[39m.resources.engine_manager = CoreEngineProcManager(\n\u001b[32m    480\u001b[39m         EngineCoreProc.run_engine_core,\n\u001b[32m    481\u001b[39m         vllm_config=vllm_config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    487\u001b[39m         start_index=start_index,\n\u001b[32m    488\u001b[39m         local_start_index=local_start_index)\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# Wait for engine core process(es) to start.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m491\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m                              \u001b[49m\u001b[43moutput_address\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:511\u001b[39m, in \u001b[36mMPClient._wait_for_engine_startup\u001b[39m\u001b[34m(self, handshake_socket, input_address, output_address)\u001b[39m\n\u001b[32m    506\u001b[39m proc_manager = \u001b[38;5;28mself\u001b[39m.resources.engine_manager\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proc_manager, (\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), CoreEngineProcManager)), (\n\u001b[32m    508\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m_wait_for_engine_startup should only be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    509\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalled with CoreEngineProcManager\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m \u001b[43mwait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcore_engines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproc_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vllm_env/lib/python3.11/site-packages/vllm/v1/utils.py:494\u001b[39m, in \u001b[36mwait_for_engine_startup\u001b[39m\u001b[34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[39m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m coord_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m coord_process.exitcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    493\u001b[39m         finished[coord_process.name] = coord_process.exitcode\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEngine core initialization failed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    495\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mSee root cause above. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    496\u001b[39m                        \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed core proc(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinished\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# Receive HELLO and READY messages from the input socket.\u001b[39;00m\n\u001b[32m    499\u001b[39m eng_identity, ready_msg_bytes = handshake_socket.recv_multipart()\n",
      "\u001b[31mRuntimeError\u001b[39m: Engine core initialization failed. See root cause above. Failed core proc(s): {}"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=1024,\n",
    "    top_k=64,\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    "    min_p=0.0\n",
    ")\n",
    "\n",
    "model = LLM(\n",
    "    model=model_slug,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    dtype=dtype,\n",
    "    max_model_len=8192\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9679ceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip freeze --color never > eval_packages.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ea21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb055b6846f4d0290ad53649cb828da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7d31eb52344307b8d77c45eaad0544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73425ec50dcb40978eec2841a85552a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing outputs:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 200 generated responses.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "outputs = model.generate(prompts, sampling_params)\n",
    "\n",
    "generated_responses_data = []\n",
    "\n",
    "# Process the outputs\n",
    "for i, output in tqdm(enumerate(outputs), total=len(outputs), desc=\"Processing outputs\"):\n",
    "    generated_text = output.outputs[0].text.strip()\n",
    "\n",
    "    generated_responses_data.append({\n",
    "        \"Context\": original_prompts[i],\n",
    "        \"True_Response\": true_responses[i],\n",
    "        \"Generated_Response\": generated_text\n",
    "    })\n",
    "\n",
    "print(f\"Finished processing {len(generated_responses_data)} generated responses.\")\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "responses_df = pd.DataFrame(generated_responses_data)\n",
    "output_csv_path = \"vllm_model_test_responses_finetuned_gemma3-4b.csv\"\n",
    "responses_df.to_csv(output_csv_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
