{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de7c69e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ardjano/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33ma-mark\u001b[0m (\u001b[33ma-mark-university-of-amsterdam\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ardjano/auf/finetune/wandb/run-20250821_230615-coaffe92</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/a-mark-university-of-amsterdam/gemma-finetuning/runs/coaffe92' target=\"_blank\">valiant-capybara-1</a></strong> to <a href='https://wandb.ai/a-mark-university-of-amsterdam/gemma-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/a-mark-university-of-amsterdam/gemma-finetuning' target=\"_blank\">https://wandb.ai/a-mark-university-of-amsterdam/gemma-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/a-mark-university-of-amsterdam/gemma-finetuning/runs/coaffe92' target=\"_blank\">https://wandb.ai/a-mark-university-of-amsterdam/gemma-finetuning/runs/coaffe92</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/a-mark-university-of-amsterdam/gemma-finetuning/runs/coaffe92?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x76058affce50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from unsloth import FastModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb # Import the wandb library\n",
    "\n",
    "# 1. Initialize WandB\n",
    "wandb.init(project=\"gemma-finetuning\", job_type=\"training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "603f7bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.13 environment at: /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env\u001b[0m\r\n",
      "\u001b[2mAudited \u001b[1m7 packages\u001b[0m \u001b[2min 8ms\u001b[0m\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!uv pip install unsloth transformers torch bitsandbytes peft accelerate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b9b73a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ardjano/.pyenv/versions/unsloth-env/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d611f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.13 environment at: /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env\u001b[0m\r\n",
      "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!uv pip install ipywidgets scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c5c0db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.13 environment at: /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env\u001b[0m\r\n",
      "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!uv pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac8ccc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env/lib/python3.11/site-packages (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env/lib/python3.11/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env/lib/python3.11/site-packages (from ipywidgets) (9.4.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env/lib/python3.11/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env/lib/python3.11/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: decorator in /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.14.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: jupyterlab-widgets in /home/ardjano/.pyenv/versions/3.11.13/envs/unsloth-env/lib/python3.11/site-packages (3.0.15)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Using pip\n",
    "!pip install -U ipywidgets\n",
    "\n",
    "# This installs the JupyterLab extension\n",
    "!pip install jupyterlab-widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3778b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "from unsloth.chat_templates import get_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24dd7357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.11: Fast Gemma3 patching. Transformers: 4.54.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.683 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71dd54fcff341f7824471a4273baa2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# more: https://huggingface.co/unsloth\n",
    "fourbit_models = [\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "]\n",
    "\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = True,\n",
    "    load_in_8bit = False,\n",
    "    full_finetuning = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a9178ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model.language_model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers = False,\n",
    "    finetune_language_layers = True,\n",
    "    finetune_attention_modules = True,\n",
    "    finetune_mlp_modules = True,\n",
    "\n",
    "    # larger = higher accuracy, but might overfit -> maybe create tabel testing various r numbers\n",
    "    # find recommendations of it\n",
    "    r = 8,\n",
    "\n",
    "    # recommended: alpha == r, at least\n",
    "    lora_alpha = 8,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad70d3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"../data/humanandllm.csv\")\n",
    "df = df[['Context', 'Response']]\n",
    "\n",
    "trainset, testset = train_test_split(df, test_size=0.057, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7158be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")\n",
    "\n",
    "def format_chat_template(row):\n",
    "    \"\"\"\n",
    "    Formats the context and response into a single string\n",
    "    using the tokenizer's chat template.\n",
    "    \"\"\"\n",
    "    # Create the conversation structure\n",
    "    row_json = [\n",
    "        {\"role\": \"user\", \"content\": row[\"Context\"]},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"Response\"]}\n",
    "    ]\n",
    "    # Apply the template and remove the beginning-of-string token\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False, add_generation_prompt=False).replace(tokenizer.bos_token, \"\")\n",
    "    return row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "304d977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the formatting function to each split DataFrame\n",
    "formatted_trainset = trainset.apply(format_chat_template, axis=1)\n",
    "formatted_testset = testset.apply(format_chat_template, axis=1)\n",
    "\n",
    "# Convert each pandas DataFrame to a Hugging Face Dataset\n",
    "hf_train = Dataset.from_pandas(formatted_trainset)\n",
    "hf_test = Dataset.from_pandas(formatted_testset)\n",
    "\n",
    "hf_train = hf_train.remove_columns([\"Context\", \"Response\"])\n",
    "hf_test = hf_test.remove_columns([\"Context\", \"Response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23af84ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f57d789c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a251af38be064632810c280f97708a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/3307 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6e45e1a79640a58be0b8bf1cfd0232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = hf_train,\n",
    "    eval_dataset  = hf_test,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field          = \"text\",\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps                = 5,\n",
    "        num_train_epochs          = 1, # maybe look at it later\n",
    "        # max_steps                   = 30,\n",
    "        learning_rate               = 2e-4,\n",
    "        logging_steps               = 5,\n",
    "\n",
    "        # evaluation\n",
    "        eval_strategy               = \"steps\",\n",
    "        eval_steps                  = 10,\n",
    "        save_strategy               = \"steps\",\n",
    "        save_steps                  = 10,\n",
    "        save_total_limit            = 2,\n",
    "\n",
    "        load_best_model_at_end      = True,\n",
    "\n",
    "        optim                       = \"adamw_8bit\",\n",
    "        weight_decay                = 0.01,\n",
    "        lr_scheduler_type           = \"linear\",\n",
    "        seed                        = 42,\n",
    "        report_to                   = \"none\",\n",
    "        dataset_num_proc            = 2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ad13467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129e0c9a37af4c80b0dcea338cee4004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/3307 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc1beab4ea44471b70496e360a7e114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c66d700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Gemma3ForConditionalGeneration does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Getting Baseline Performance ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 04:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.4750258922576904, 'eval_model_preparation_time': 0.0125, 'eval_runtime': 90.4801, 'eval_samples_per_second': 2.21, 'eval_steps_per_second': 0.553}\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Getting Baseline Performance ---\")\n",
    "baseline_metrics = trainer.evaluate()\n",
    "print(baseline_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1d5fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_eval_loss_at_step_0 = baseline_metrics['eval_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d0dbc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch._dynamo.config.cache_size_limit = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7437201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 3,307 | Num Epochs = 1 | Total steps = 414\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 56,758,272 of 27,489,164,912 (0.21% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='59' max='414' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 59/414 18:11 < 1:53:16, 0.05 it/s, Epoch 0.14/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.558700</td>\n",
       "      <td>2.399634</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.108400</td>\n",
       "      <td>2.297781</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.249500</td>\n",
       "      <td>2.272112</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.214200</td>\n",
       "      <td>2.256271</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.998500</td>\n",
       "      <td>2.248016</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2659c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bedbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the object from the file\n",
    "with open(\"trainer_stats.pkl\", \"rb\") as f:\n",
    "    loaded_stats = pickle.load(f)\n",
    "\n",
    "print(loaded_stats)\n",
    "# Now you can use it just like the original object\n",
    "print(f\"Total training loss: {loaded_stats.training_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39990d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_eval_loss_at_step_0 = 2.8692877292633057"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780587b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Apply the professional seaborn theme\n",
    "sns.set_theme(style=\"whitegrid\", context=\"paper\")\n",
    "\n",
    "log_history = trainer.state.log_history\n",
    "train_logs = [log for log in log_history if 'loss' in log]\n",
    "eval_logs = [log for log in log_history if 'eval_loss' in log]\n",
    "\n",
    "train_steps = [log['step'] for log in train_logs]\n",
    "train_losses = [log['loss'] for log in train_logs]\n",
    "eval_steps = [log['step'] for log in eval_logs]\n",
    "eval_losses = [log['eval_loss'] for log in eval_logs]\n",
    "\n",
    "eval_steps.insert(0, 0)\n",
    "eval_losses.insert(0, baseline_eval_loss_at_step_0)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# sns.lineplot for styled lines\n",
    "sns.lineplot(x=train_steps, y=train_losses, label='Training Loss')\n",
    "sns.lineplot(x=eval_steps, y=eval_losses, label='Validation Loss', marker='o', linestyle='--')\n",
    "\n",
    "plt.title('Training vs. Validation Loss')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('gemma27b_loss1.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645d2e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd7728",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"../trained_models/gemma3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b35776",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained_merged(\n",
    "    \"../trained_models/gemma3-27b-finetuned-16bitv2\",  # A new folder name for the final model\n",
    "    tokenizer,\n",
    "    save_method = \"merged_16bit\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
